"""
æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–è¿ç§»
ä¸ºç©ºé—´æŸ¥è¯¢å’Œé«˜é¢‘æŸ¥è¯¢åˆ›å»ºæ€§èƒ½ç´¢å¼•
"""

import asyncio
import logging
from typing import List, Dict, Any
from datetime import datetime
import asyncpg
from src.core.connection_pool import get_postgres_pool

logger = logging.getLogger(__name__)

class SpatialIndexOptimizer:
    """ç©ºé—´ç´¢å¼•ä¼˜åŒ–å™¨"""

    # ç©ºé—´ç´¢å¼•å®šä¹‰
    SPATIAL_INDEXES = [
        {
            "table": "monitoring_stations",
            "column": "geom",
            "index_type": "GIST",
            "name": "idx_monitoring_stations_geom",
            "condition": "geom IS NOT NULL",
            "description": "ç›‘æµ‹ç«™ç©ºé—´ä½ç½®ç´¢å¼•"
        },
        {
            "table": "water_level_stations",
            "column": "geom",
            "index_type": "GIST",
            "name": "idx_water_level_stations_geom",
            "condition": "geom IS NOT NULL",
            "description": "æ°´ä½ç«™ç©ºé—´ä½ç½®ç´¢å¼•"
        },
        {
            "table": "flood_risk_areas",
            "column": "location",
            "index_type": "GIST",
            "name": "idx_flood_risk_areas_location",
            "condition": "location IS NOT NULL",
            "description": "æ´ªæ°´é£é™©åŒºåŸŸç©ºé—´ç´¢å¼•"
        },
        {
            "table": "spatial_features",
            "column": "geom",
            "index_type": "GIST",
            "name": "idx_spatial_features_geom",
            "condition": "geom IS NOT NULL",
            "description": "ç©ºé—´è¦ç´ é€šç”¨ç´¢å¼•"
        },
        {
            "table": "historical_flood_events",
            "column": "geom",
            "index_type": "GIST",
            "name": "idx_historical_flood_events_geom",
            "condition": "geom IS NOT NULL",
            "description": "å†å²æ´ªæ°´äº‹ä»¶ç©ºé—´ç´¢å¼•"
        }
    ]

    # å¤åˆç´¢å¼•å®šä¹‰
    COMPOSITE_INDEXES = [
        {
            "table": "monitoring_stations",
            "columns": ["type", "status", "last_updated"],
            "name": "idx_monitoring_stations_type_status_updated",
            "description": "ç›‘æµ‹ç«™ç±»å‹çŠ¶æ€æ—¶é—´å¤åˆç´¢å¼•"
        },
        {
            "table": "flood_risk_areas",
            "columns": ["severity", "last_updated"],
            "name": "idx_flood_risk_severity_updated",
            "description": "æ´ªæ°´é£é™©ç­‰çº§æ—¶é—´å¤åˆç´¢å¼•"
        },
        {
            "table": "water_level_stations",
            "columns": ["status", "current_level", "alert_level"],
            "name": "idx_water_level_status_levels",
            "description": "æ°´ä½ç«™çŠ¶æ€å’Œç­‰çº§å¤åˆç´¢å¼•"
        },
        {
            "table": "messages",
            "columns": ["conversation_id", "created_at"],
            "name": "idx_messages_conversation_created",
            "description": "æ¶ˆæ¯ä¼šè¯æ—¶é—´å¤åˆç´¢å¼•"
        },
        {
            "table": "layers",
            "columns": ["map_id", "type", "visible"],
            "name": "idx_layers_map_type_visible",
            "description": "å›¾å±‚åœ°å›¾ç±»å‹å¯è§æ€§å¤åˆç´¢å¼•"
        }
    ]

    # å•åˆ—ç´¢å¼•å®šä¹‰
    SINGLE_COLUMN_INDEXES = [
        {
            "table": "users",
            "column": "email",
            "name": "idx_users_email_unique",
            "unique": True,
            "description": "ç”¨æˆ·é‚®ç®±å”¯ä¸€ç´¢å¼•"
        },
        {
            "table": "users",
            "column": "username",
            "name": "idx_users_username_unique",
            "unique": True,
            "description": "ç”¨æˆ·åå”¯ä¸€ç´¢å¼•"
        },
        {
            "table": "projects",
            "column": "user_id",
            "name": "idx_projects_user_id",
            "description": "é¡¹ç›®ç”¨æˆ·IDç´¢å¼•"
        },
        {
            "table": "maps",
            "column": "project_id",
            "name": "idx_maps_project_id",
            "description": "åœ°å›¾é¡¹ç›®IDç´¢å¼•"
        },
        {
            "table": "conversations",
            "column": "user_id",
            "name": "idx_conversations_user_id",
            "description": "ä¼šè¯ç”¨æˆ·IDç´¢å¼•"
        },
        {
            "table": "project_postgres_connections",
            "column": "user_id",
            "name": "idx_connections_user_id",
            "description": "æ•°æ®åº“è¿æ¥ç”¨æˆ·IDç´¢å¼•"
        }
    ]

    # æ—¶é—´åºåˆ—ç´¢å¼•
    TIME_SERIES_INDEXES = [
        {
            "table": "water_level_measurements",
            "column": "measurement_time",
            "name": "idx_water_level_measurement_time",
            "description": "æ°´ä½æµ‹é‡æ—¶é—´ç´¢å¼•"
        },
        {
            "table": "flood_events",
            "column": "occurrence_date",
            "name": "idx_flood_events_date",
            "description": "æ´ªæ°´äº‹ä»¶æ—¥æœŸç´¢å¼•"
        },
        {
            "table": "messages",
            "column": "created_at",
            "name": "idx_messages_created_at",
            "description": "æ¶ˆæ¯åˆ›å»ºæ—¶é—´ç´¢å¼•"
        }
    ]

    def __init__(self, conn: asyncpg.Connection):
        self.conn = conn

    async def check_index_exists(self, index_name: str) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨"""
        try:
            result = await self.conn.fetchrow("""
                SELECT 1 FROM pg_indexes WHERE indexname = $1
            """, index_name)
            return result is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç´¢å¼•å­˜åœ¨æ€§å¤±è´¥ {index_name}: {e}")
            return False

    async def create_spatial_index(self, index_config: Dict[str, Any]) -> bool:
        """åˆ›å»ºç©ºé—´ç´¢å¼•"""
        index_name = index_config["name"]
        table = index_config["table"]
        column = index_config["column"]
        index_type = index_config["index_type"]
        condition = index_config.get("condition", "")

        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            if await self.check_index_exists(index_name):
                logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True

            # æ„å»ºç´¢å¼•åˆ›å»ºSQL
            sql = f"""
                CREATE INDEX CONCURRENTLY IF NOT EXISTS {index_name}
                ON {table} USING {index_type} ({column})
            """

            if condition:
                sql += f" WHERE {condition}"

            logger.info(f"åˆ›å»ºç©ºé—´ç´¢å¼•: {index_name} - {index_config['description']}")
            await self.conn.execute(sql)
            logger.info(f"âœ… ç©ºé—´ç´¢å¼• {index_name} åˆ›å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åˆ›å»ºç©ºé—´ç´¢å¼• {index_name} å¤±è´¥: {e}")
            return False

    async def create_composite_index(self, index_config: Dict[str, Any]) -> bool:
        """åˆ›å»ºå¤åˆç´¢å¼•"""
        index_name = index_config["name"]
        table = index_config["table"]
        columns = index_config["columns"]

        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            if await self.check_index_exists(index_name):
                logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True

            # æ„å»ºåˆ—åˆ—è¡¨
            columns_str = ", ".join(columns)

            sql = f"""
                CREATE INDEX CONCURRENTLY IF NOT EXISTS {index_name}
                ON {table} ({columns_str})
            """

            logger.info(f"åˆ›å»ºå¤åˆç´¢å¼•: {index_name} - {index_config['description']}")
            await self.conn.execute(sql)
            logger.info(f"âœ… å¤åˆç´¢å¼• {index_name} åˆ›å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åˆ›å»ºå¤åˆç´¢å¼• {index_name} å¤±è´¥: {e}")
            return False

    async def create_single_column_index(self, index_config: Dict[str, Any]) -> bool:
        """åˆ›å»ºå•åˆ—ç´¢å¼•"""
        index_name = index_config["name"]
        table = index_config["table"]
        column = index_config["column"]
        unique = index_config.get("unique", False)

        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            if await self.check_index_exists(index_name):
                logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True

            unique_str = "UNIQUE " if unique else ""
            sql = f"""
                CREATE {unique_str}INDEX CONCURRENTLY IF NOT EXISTS {index_name}
                ON {table} ({column})
            """

            logger.info(f"åˆ›å»ºå•åˆ—ç´¢å¼•: {index_name} - {index_config['description']}")
            await self.conn.execute(sql)
            logger.info(f"âœ… å•åˆ—ç´¢å¼• {index_name} åˆ›å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åˆ›å»ºå•åˆ—ç´¢å¼• {index_name} å¤±è´¥: {e}")
            return False

    async def create_time_series_index(self, index_config: Dict[str, Any]) -> bool:
        """åˆ›å»ºæ—¶é—´åºåˆ—ç´¢å¼•"""
        index_name = index_config["name"]
        table = index_config["table"]
        column = index_config["column"]

        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            if await self.check_index_exists(index_name):
                logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True

            # æ—¶é—´åºåˆ—ç´¢å¼•é€šå¸¸éœ€è¦BRINç´¢å¼•ä»¥æé«˜æ€§èƒ½
            sql = f"""
                CREATE INDEX CONCURRENTLY IF NOT EXISTS {index_name}
                ON {table} USING BRIN ({column})
            """

            logger.info(f"åˆ›å»ºæ—¶é—´åºåˆ—ç´¢å¼•: {index_name} - {index_config['description']}")
            await self.conn.execute(sql)
            logger.info(f"âœ… æ—¶é—´åºåˆ—ç´¢å¼• {index_name} åˆ›å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åˆ›å»ºæ—¶é—´åºåˆ—ç´¢å¼• {index_name} å¤±è´¥: {e}")
            return False

    async def analyze_table_stats(self, table_name: str) -> bool:
        """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            logger.info(f"åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯: {table_name}")
            await self.conn.execute(f"ANALYZE {table_name}")
            logger.info(f"âœ… è¡¨ {table_name} ç»Ÿè®¡ä¿¡æ¯æ›´æ–°æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ {table_name}: {e}")
            return False

    async def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–ç´¢å¼•ä½¿ç”¨æƒ…å†µç»Ÿè®¡
            index_stats = await self.conn.fetch("""
                SELECT
                    schemaname,
                    tablename,
                    indexname,
                    indexdef,
                    tablespace,
                    indexprs,
                    indpred,
                    indisunique,
                    indisprimary,
                    indisexclusion,
                    indimmediate,
                    indisclustered,
                    indisvalid,
                    indcheckxmin,
                    indisready,
                    indislive,
                    indisreplident,
                    indoption,
                    indexprs,
                    indpred
                FROM pg_indexes
                WHERE schemaname = 'public'
                ORDER BY tablename, indexname
            """)

            # è·å–ç´¢å¼•å¤§å°ç»Ÿè®¡
            index_sizes = await self.conn.fetch("""
                SELECT
                    indexname,
                    pg_size_pretty(pg_relation_size(indexname::regclass)) as size
                FROM pg_indexes
                WHERE schemaname = 'public'
            """)

            return {
                "total_indexes": len(index_stats),
                "index_list": [dict(row) for row in index_stats],
                "index_sizes": [dict(row) for row in index_sizes],
                "timestamp": datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.error(f"è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}

    async def create_all_indexes(self) -> Dict[str, Any]:
        """åˆ›å»ºæ‰€æœ‰ç´¢å¼•"""
        logger.info("ğŸš€ å¼€å§‹åˆ›å»ºæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–...")

        results = {
            "spatial_indexes": {"created": 0, "failed": 0, "skipped": 0, "details": []},
            "composite_indexes": {"created": 0, "failed": 0, "skipped": 0, "details": []},
            "single_column_indexes": {"created": 0, "failed": 0, "skipped": 0, "details": []},
            "time_series_indexes": {"created": 0, "failed": 0, "skipped": 0, "details": []},
            "start_time": datetime.utcnow(),
            "end_time": None
        }

        # 1. åˆ›å»ºç©ºé—´ç´¢å¼•
        logger.info("ğŸ—ºï¸ åˆ›å»ºç©ºé—´ç´¢å¼•...")
        for index_config in self.SPATIAL_INDEXES:
            success = await self.create_spatial_index(index_config)
            if success:
                results["spatial_indexes"]["created"] += 1
                results["spatial_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "created",
                    "description": index_config["description"]
                })
            else:
                results["spatial_indexes"]["failed"] += 1
                results["spatial_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "failed",
                    "description": index_config["description"]
                })

        # 2. åˆ›å»ºå¤åˆç´¢å¼•
        logger.info("ğŸ”— åˆ›å»ºå¤åˆç´¢å¼•...")
        for index_config in self.COMPOSITE_INDEXES:
            success = await self.create_composite_index(index_config)
            if success:
                results["composite_indexes"]["created"] += 1
                results["composite_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "created",
                    "description": index_config["description"]
                })
            else:
                results["composite_indexes"]["failed"] += 1
                results["composite_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "failed",
                    "description": index_config["description"]
                })

        # 3. åˆ›å»ºå•åˆ—ç´¢å¼•
        logger.info("ğŸ“Š åˆ›å»ºå•åˆ—ç´¢å¼•...")
        for index_config in self.SINGLE_COLUMN_INDEXES:
            success = await self.create_single_column_index(index_config)
            if success:
                results["single_column_indexes"]["created"] += 1
                results["single_column_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "created",
                    "description": index_config["description"]
                })
            else:
                results["single_column_indexes"]["failed"] += 1
                results["single_column_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "failed",
                    "description": index_config["description"]
                })

        # 4. åˆ›å»ºæ—¶é—´åºåˆ—ç´¢å¼•
        logger.info("â° åˆ›å»ºæ—¶é—´åºåˆ—ç´¢å¼•...")
        for index_config in self.TIME_SERIES_INDEXES:
            success = await self.create_time_series_index(index_config)
            if success:
                results["time_series_indexes"]["created"] += 1
                results["time_series_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "created",
                    "description": index_config["description"]
                })
            else:
                results["time_series_indexes"]["failed"] += 1
                results["time_series_indexes"]["details"].append({
                    "name": index_config["name"],
                    "status": "failed",
                    "description": index_config["description"]
                })

        # 5. åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
        logger.info("ğŸ“ˆ åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯...")
        tables_to_analyze = set()
        for index_list in [self.SPATIAL_INDEXES, self.COMPOSITE_INDEXES,
                          self.SINGLE_COLUMN_INDEXES, self.TIME_SERIES_INDEXES]:
            for index_config in index_list:
                tables_to_analyze.add(index_config["table"])

        for table in tables_to_analyze:
            await self.analyze_table_stats(table)

        # 6. è·å–ç´¢å¼•ç»Ÿè®¡
        logger.info("ğŸ“Š è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯...")
        index_stats = await self.get_index_stats()

        results["end_time"] = datetime.utcnow()
        results["index_stats"] = index_stats
        results["duration_seconds"] = (results["end_time"] - results["start_time"]).total_seconds()

        # æ€»ç»“
        total_created = (results["spatial_indexes"]["created"] +
                        results["composite_indexes"]["created"] +
                        results["single_column_indexes"]["created"] +
                        results["time_series_indexes"]["created"])

        total_failed = (results["spatial_indexes"]["failed"] +
                       results["composite_indexes"]["failed"] +
                       results["single_column_indexes"]["failed"] +
                       results["time_series_indexes"]["failed"])

        logger.info(f"ğŸ‰ ç´¢å¼•ä¼˜åŒ–å®Œæˆ!")
        logger.info(f"   æˆåŠŸåˆ›å»º: {total_created} ä¸ªç´¢å¼•")
        logger.info(f"   å¤±è´¥: {total_failed} ä¸ªç´¢å¼•")
        logger.info(f"   æ€»è€—æ—¶: {results['duration_seconds']:.2f} ç§’")

        return results

# ä¾¿æ·çš„è¿ç§»å‡½æ•°
async def migrate_spatial_indexes():
    """æ‰§è¡Œç©ºé—´ç´¢å¼•è¿ç§»"""
    logger.info("å¼€å§‹ç©ºé—´ç´¢å¼•ä¼˜åŒ–è¿ç§»...")

    try:
        # è·å–æ•°æ®åº“è¿æ¥
        conn = get_postgres_pool()
        optimizer = SpatialIndexOptimizer(conn)

        # æ‰§è¡Œç´¢å¼•åˆ›å»º
        results = await optimizer.create_all_indexes()

        # è®°å½•ç»“æœ
        logger.info("ç©ºé—´ç´¢å¼•è¿ç§»å®Œæˆ")
        return results

    except Exception as e:
        logger.error(f"ç©ºé—´ç´¢å¼•è¿ç§»å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    # è¿è¡Œè¿ç§»
    async def main():
        results = await migrate_spatial_indexes()
        print(json.dumps(results, indent=2, default=str))

    asyncio.run(main())